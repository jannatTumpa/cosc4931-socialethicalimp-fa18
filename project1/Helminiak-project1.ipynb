{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Program: Helminiak-project1\n",
    "#Version: 1.0\n",
    "#Author: David Helminiak\n",
    "#Date Created: 12 October 2018\n",
    "#Date Last Modified: 18 October 2018\n",
    "#Changelog: 0.1 - visual decision tree construction - Oct 12, 2018\n",
    "#           0.2 - accepts generic dataset and uses parallel processing - Oct 15, 2018\n",
    "#           0.3 - changes un-balanced class dataset to balanced; optimizes split function - Oct 15, 2018\n",
    "#           0.4 - balances classes using normal, truncated_normal, or oversampling techniques - Oct 15, 2018\n",
    "#           0.5 - provides basic statistical analysis - Oct 16, 2018\n",
    "#           0.6 - drops low class. corr. variables, draws from balanced df for training, term. tree gen on 1 rem. class - Oct 17, 2018\n",
    "#           0.7 - fixed failure to evaluate any but the first variable's information gain - Oct 18, 2018\n",
    "#           0.8 - quick function run options - Oct 18, 2018\n",
    "#           0.9 - print results to file - Oct 18, 2018\n",
    "#           1.0 - Finished initial contruction- Oct 18, 2018\n",
    "#USEFUL FUNCTIONS:\n",
    "#Add Breakpoint: from IPython.core.debugger import Tracer; Tracer()() \n",
    "\n",
    "#LIBRARY IMPORTS\n",
    "import sys, os, math, pydot, multiprocessing, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import Image\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "#Also requires installation of GraphViz package - for OSX: \"brew install graphviz\"\n",
    "\n",
    "#FUNCTIONS AND CLASS DEFINITIONS\n",
    "\n",
    "#Calculate gini index for a particular characteristic\n",
    "def gini(df, column):\n",
    "        gini=1\n",
    "        #Get count of number of rows\n",
    "        rowCount = (len(df))\n",
    "        #Get list and count of unique values\n",
    "        countVals = df[column].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "        for i in range(0, (len(countVals))):\n",
    "            gini=gini-(countVals.iloc[i,1]/rowCount)**2   \n",
    "        return gini\n",
    "\n",
    "#Determine split point information gain results for each projection value\n",
    "def bestSplit_parhelper(i, df, originalGini, characteristic, classifierID, projectVals, splits):\n",
    "    #Load dataframe into left and right nodes\n",
    "    leftData = pd.DataFrame(columns = df.columns.values)\n",
    "    rightData = pd.DataFrame(columns = df.columns.values)\n",
    "   \n",
    "    for j in range(0, len(df)): #For the length of the dataframe\n",
    "        if (df[characteristic].iloc[j] < projectVals[i]): #For any values less than projectVals[i]\n",
    "            leftData=leftData.append(df.iloc[j], ignore_index=True)\n",
    "        else: #Otherwise, values are greater than or equal to projected value\n",
    "            rightData=rightData.append(df.iloc[j], ignore_index=True)\n",
    "    #Calculate gini values for left and right nodes\n",
    "    leftGini=gini(leftData, classifierID)\n",
    "    rightGini=gini(rightData, classifierID)\n",
    "    #Calculate information gain and append to splits df\n",
    "    combinedGini=((len(leftData)/len(df))*leftGini)+((len(rightData)/len(df))*rightGini)\n",
    "    informationGain=originalGini-combinedGini\n",
    "    splits['information_gain'].iloc[i]=informationGain\n",
    "    return splits\n",
    "\n",
    "#Determine the best possible information gain and splitting point for a characteristic\n",
    "def bestSplit(df, originalGini, characteristic, classifierID):\n",
    "    #Get list and count of unique values\n",
    "    countVals = df[characteristic].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "    countVals = countVals.sort_values(by='unique_values') #Sort countVals by values rather than count\n",
    "    #Project mean values to find candidate splitting points\n",
    "    projectVals=[]\n",
    "    for i in range(0, len(countVals['unique_values'])-1):\n",
    "        projectVals.append((countVals['unique_values'].iloc[i]+countVals['unique_values'].iloc[i+1])/2)\n",
    "    #Test data splits\n",
    "    splits = pd.DataFrame(data={'projection_values': projectVals, 'information_gain': np.nan})\n",
    "    splitsCompiled = pd.DataFrame(data={'projection_values': projectVals, 'information_gain': np.nan})\n",
    "    #For each of the possible splitting points calculate the resulting information gain\n",
    "    num_threads = multiprocessing.cpu_count() #Determine number of available threads\n",
    "    splits = Parallel(n_jobs=num_threads)(delayed(bestSplit_parhelper)(i, df, originalGini, characteristic, classifierID, projectVals, splits) for i in range(0, len(projectVals))) #Perform task in parallel\n",
    "    #Splits returns as a list with every ith row's ith value being the next value desired\n",
    "    #Transform splits list back into dataframe\n",
    "    for i in range (0, len(splits)):\n",
    "        splitsCompiled['information_gain'].iloc[i] = splits[i].iloc[i]['information_gain']\n",
    "    #Locate the best split point if there is one\n",
    "    if (len(splitsCompiled) is 0): #If there is no data to split\n",
    "        return 0, 0 #Then there is no information to be gained and the split point is negligable\n",
    "    splitPoint=splitsCompiled['projection_values'].iloc[splitsCompiled['information_gain'].idxmax()]\n",
    "    maxGain = splitsCompiled['information_gain'].value_counts().idxmax()\n",
    "    return maxGain, splitPoint\n",
    "\n",
    "#Find best information gain over all of the characteristics and then split the data accordingly\n",
    "def split(df, classifierID, printEverything):\n",
    "    #Calculate original gini\n",
    "    originalGini = gini(df, classifierID)\n",
    "\n",
    "    #Get characteristic names\n",
    "    columnNames=list(df.columns.values)\n",
    "    columnNames.remove(columnNames[len(columnNames)-1])\n",
    "\n",
    "    #Determine which is best to perform split\n",
    "    charSplit = pd.DataFrame(data={'characteristic': columnNames, 'information_gain': np.nan, 'splitting_point': np.nan})\n",
    "    for i in range (0, len(columnNames)): \n",
    "        print('Split Evaluation: ', i/len(columnNames)*100, '%')\n",
    "        charInformationGain, charSplitPoint = bestSplit(df, originalGini, columnNames[i], classifierID)\n",
    "        charSplit['information_gain'].iloc[i]=charInformationGain\n",
    "        charSplit['splitting_point'].iloc[i]=charSplitPoint\n",
    "    splitChar=charSplit['characteristic'].iloc[charSplit['information_gain'].idxmax()]\n",
    "    splitPoint=charSplit['splitting_point'].iloc[charSplit['information_gain'].idxmax()]\n",
    "\n",
    "    #Actually split the data\n",
    "    #Load dataframe into left and right nodes\n",
    "    leftData = df.copy()\n",
    "    rightData = df.copy()\n",
    "    for i in range(0, len(df)): #For the length of the dataframe\n",
    "        if (rightData[splitChar].iloc[i] < splitPoint): #For any values less than projectVals[i]\n",
    "            rightData[splitChar].iloc[i] = np.nan #Set row in right side as NaN\n",
    "        else: #Otherwise, values are greater than or equal to projected value\n",
    "            leftData[splitChar].iloc[i] = np.nan #Set row in left side as NaN\n",
    "    #Delete rows with nan values for both left and right side\n",
    "    leftData=leftData.dropna()\n",
    "    rightData=rightData.dropna()\n",
    "    return splitChar, splitPoint, leftData, rightData\n",
    "\n",
    "#Build the full tree from each sub-tree found for each node within a decision tree object\n",
    "def buildGraph(tree):\n",
    "    finalGraph = pydot.Dot(graph_type='graph') #Create a blank tree to hold all sub-trees\n",
    "    root = tree.graph #Establish the tree's root sub-tree\n",
    "    for i in range(0,len(root.get_edges())):\n",
    "        finalGraph.add_edge(root.get_edges()[i])\n",
    "    if (tree.leftChild is not None): #If there is a further sub-tree\n",
    "        a = buildGraph(tree.leftChild) #Recursive call for left hand child \n",
    "        for i in range(0,len(a.get_edges())): #For all of the left hand child's edges\n",
    "            finalGraph.add_edge(a.get_edges()[i])  #Add them to the final graph\n",
    "        b = buildGraph(tree.rightChild) #Recursive call for right hand child \n",
    "        for i in range(0,len(b.get_edges())): #For all of the right hand child's edges\n",
    "            finalGraph.add_edge(b.get_edges()[i]) #Add them to the final tree\n",
    "    return finalGraph #Return back up the final tree\n",
    "\n",
    "#Determine what the tree says the classifier ID should be\n",
    "def determine(startNode, dataPoint, classifierID):\n",
    "    if (startNode.leftChild is not None):\n",
    "        if (dataPoint[startNode.splitChar]<startNode.splitPoint):\n",
    "            startNode = startNode.leftChild\n",
    "        else:\n",
    "            startNode = startNode.rightChild\n",
    "        if (startNode.leftChild is not None):\n",
    "            startNode = determine(startNode, dataPoint, classifierID)\n",
    "        return startNode\n",
    "    \n",
    "#Test if a correct answer is obtained through the decision tree for a sample\n",
    "def test(tree, testData, classifierID):\n",
    "    successes=0\n",
    "    for i in range (0,len(testData)): #For each of the test data cases\n",
    "        if (testData.iloc[i][classifierID] == determine(tree, testData.iloc[i], classifierID).classifierID):\n",
    "            successes=successes+1\n",
    "    return successes \n",
    "\n",
    "def distribute(df, columnNames, n, oversample, synthetic, truncated_normal, normal):\n",
    "    random.seed(datetime.now())\n",
    "    characteristic_value_sets = np.zeros(((len(columnNames)),n))  #Create array to hold the generated characteristic value sets\n",
    "    for m in range (0, len(columnNames)): #For each of the characteristics \n",
    "        distributed_values=[] #Create an empty list to hold sample values\n",
    "        #Calculate base statistics\n",
    "        mean = df[columnNames[m]].mean()\n",
    "        sigma = df[columnNames[m]].var()\n",
    "        if (sigma >= 0):\n",
    "            sigma=sigma\n",
    "        else:\n",
    "            sigma = 0\n",
    "        minimum = df[columnNames[m]].min()\n",
    "        maximum = df[columnNames[m]].max()\n",
    "        if (sigma != 0): #If there is varience in the values provided\n",
    "            if (oversample == 1): #If compensating unbalance using oversampling technique\n",
    "                #Generate a random sample of n values from this distribution\n",
    "                for p in range (0, n): #For the number of samples\n",
    "                    rand = round((random.random()*(len(df[columnNames[m]])-1)-1)+1)                    \n",
    "                    distributed_values.append(df[columnNames[m]].tolist()[rand])\n",
    "            elif (synthetic == 1): #If compensating unbalance using synthetic value generation\n",
    "                if (truncated_normal == 1): #If sampling should be performed from truncated normal distribution\n",
    "                     distributed_values = stats.truncnorm.rvs((minimum-mean)/sigma, (maximum-mean)/sigma, scale=sigma, loc=mean, size=n)\n",
    "                elif (normal == 1): #If sampling should be performed from a normal distribution\n",
    "                    distributed_values = np.random.normal(mean, sigma, n)\n",
    "        else: #Otherwise use values equal to the mean\n",
    "            distributed_values = [mean] * n\n",
    "        characteristic_value_sets[m]=distributed_values\n",
    "    return characteristic_value_sets\n",
    "\n",
    "#Create normal distribution of variable data for an unbalanced dataset given its end classifier's identification\n",
    "def balanceData(df, toBalanceData, classifierID, n, oversample, synthetic, truncated_normal, normal):\n",
    "    #Find unique values and counts of such in the training data\n",
    "    training_counts = toBalanceData[classifierID].value_counts().rename_axis('unique_values').reset_index(name='counts').sort_values(by='unique_values')\n",
    "    \n",
    "    #Get characteristic names\n",
    "    columnNames=list(df.columns.values)\n",
    "    columnNames.remove(columnNames[len(columnNames)-1])\n",
    "    #Determine binary characteristics\n",
    "    binary_characteristics = [] #Create empty list to hold any characteristics that have binary values\n",
    "    #For each of the characteristics\n",
    "    for i in range (0, len(columnNames)):\n",
    "        #If in the whole dataset only have 2 unique values, then they are binary\n",
    "        if ((len(df[columnNames[i]].value_counts()))==2):\n",
    "            binary_characteristics.append(Binary_Characteristic(columnNames[i], df[columnNames[i]].value_counts().rename_axis('unique_values').reset_index(name='counts').sort_values(by='unique_values')['unique_values']))\n",
    "    if (n == -1): #If number of samples is set to automatic\n",
    "        n = len(toBalanceData)/(len(training_counts['unique_values'])) #Make the number of samples prop. to original samples and classifiers\n",
    "    if n % 2 != 0: #If the sample size is not even\n",
    "        n = n+1 #Add 1 to make it so\n",
    "    original_sample_size = n #Make a backup of the starting sample size\n",
    "    balanced_training_data=pd.DataFrame(columns = df.columns.values)\n",
    "    if (len(binary_characteristics) != 0): #If there even is a binary characteristic\n",
    "        for i in range (0, len(binary_characteristics)): #For each of the binary characteristics\n",
    "            binary_characteristic = binary_characteristics[i] #Load the characteristic data\n",
    "            for j in range (0, len(binary_characteristic.values)): #For each of the binary characteristic's values; should always be 2\n",
    "                binary_data = toBalanceData[toBalanceData[binary_characteristic.label] == binary_characteristic.values[j]] #Load training data for the binary characteristic label's value \n",
    "                #Find unique values and counts of such in the training data\n",
    "                unique_counts = binary_data[classifierID].value_counts().rename_axis('unique_values').reset_index(name='counts').sort_values(by='unique_values')\n",
    "                for k in unique_counts['unique_values']: #For each of the unique classes\n",
    "                    binary_class_data = binary_data[binary_data[classifierID] == k] #Load data specific to a single class\n",
    "                    #If a class is unique to the binary value, then the number of samples generated should compensate\n",
    "                    n = original_sample_size #Reset the sample size\n",
    "                    binary_variable_count = 0 #Counter for the number of binary variables that hold the class\n",
    "                    for p in range (0, len(binary_characteristics)): #For each of the binary characteristics\n",
    "                        binary_characteristic_2 = binary_characteristics[i] #Load the characteristic data \n",
    "                        for q in range (0, len(binary_characteristic.values)): #For each of the binary characteristic's values; should always be 2\n",
    "                            binary_data_2 = toBalanceData[toBalanceData[binary_characteristic_2.label] == binary_characteristic_2.values[q]] #Load training data for the binary characteristic label's value\n",
    "                            if (len(binary_data_2[binary_data_2[classifierID] == k][classifierID].value_counts()) != 0): #If the class value is present for a binary characteristic's value\n",
    "                                binary_variable_count = binary_variable_count+1 #Increase the count by 1\n",
    "                    for p in range(1, binary_variable_count): #For each value of the count\n",
    "                        n = n - (n/2) #Subtract half of n's current value\n",
    "                    n = round(n) #Round the value for indexing\n",
    "                    #Distribute \n",
    "                    characteristic_value_sets = distribute(binary_class_data, columnNames, n, oversample, synthetic, truncated_normal, normal)\n",
    "                    \n",
    "                    #Form distributed values into a new dataframe and add it to the balanced dataset\n",
    "                    balanced_characteristics = pd.DataFrame(np.transpose(characteristic_value_sets), columns = columnNames)\n",
    "                    balanced_characteristics[classifierID] = k\n",
    "                    balanced_training_data = balanced_training_data.append(balanced_characteristics, ignore_index=True)\n",
    "    else: #If there are no binary characteristics\n",
    "        n = round(n) #Round the value for indexing\n",
    "        characteristic_value_sets = np.zeros((len(columnNames),n))  #Create array to hold the generated characteristic value sets\n",
    "        unique_counts = toBalanceData[classifierID].value_counts().rename_axis('unique_values').reset_index(name='counts').sort_values(by='unique_values')\n",
    "        \n",
    "        for k in unique_counts['unique_values']: #For each of the unique classes\n",
    "            class_data = toBalanceData[toBalanceData[classifierID] == k]\n",
    "            characteristic_value_sets = distribute(class_data, columnNames, n, oversample, synthetic, truncated_normal, normal)\n",
    "            balanced_characteristics = pd.DataFrame(np.transpose(characteristic_value_sets), columns = columnNames)\n",
    "            balanced_characteristics[classifierID] = k\n",
    "            balanced_training_data = balanced_training_data.append(balanced_characteristics, ignore_index=True)     \n",
    "    return balanced_training_data\n",
    "\n",
    "#Define a descision tree object\n",
    "class Decision_Tree:\n",
    "    def __init__(self, df, identifier, depth, printEverything):\n",
    "        #Define graph\n",
    "        self.graph = pydot.Dot(graph_type='graph')\n",
    "        #Define internal node variables\n",
    "        self.data=df\n",
    "        self.classifierID=df[identifier].value_counts().idxmax()\n",
    "        self.depth = depth\n",
    "        self.gini=gini(df, identifier)\n",
    "        self.printEverything=printEverything\n",
    "        if (len(self.data[classifierID].value_counts()) > 1): #If the data has more than 1 classifier remaining\n",
    "            print ('Depth: '+str(self.depth))\n",
    "            print('Splitting with gini:',self.gini,'and:',len(self.data),'samples')\n",
    "            self.splitChar, self.splitPoint, self.leftChildData, self.rightChildData = split(df, classifierID, self.printEverything)\n",
    "            if ((len(self.leftChildData) == 0) or (len(self.rightChildData) == 0)):\n",
    "                self.leftChild = None\n",
    "                self.rightChild = None\n",
    "            else:\n",
    "                self.leftChild = Decision_Tree(self.leftChildData, identifier, (self.depth+1), printEverything)\n",
    "                self.rightChild = Decision_Tree(self.rightChildData, identifier, (self.depth+1), printEverything)\n",
    "        else:\n",
    "            self.leftChild = None\n",
    "            self.rightChild = None\n",
    "        #Define nodal information\n",
    "        self.nodeInformation='Mode: '+str(self.classifierID)\n",
    "        self.nodeInformation=self.nodeInformation+'\\nNumber of Members: '+str(len(self.data))\n",
    "        self.nodeInformation=self.nodeInformation+'\\nGini: '+str(self.gini)\n",
    "        if (self.leftChild is not None):\n",
    "            self.nodeInformation = self.nodeInformation+'\\n(Left): '+self.splitChar+'<'+str(self.splitPoint)\n",
    "            self.leftEdge = pydot.Edge(self.nodeInformation, self.leftChild.nodeInformation)\n",
    "            self.graph.add_edge(self.leftEdge)\n",
    "            self.rightEdge = pydot.Edge(self.nodeInformation, self.rightChild.nodeInformation)\n",
    "            self.graph.add_edge(self.rightEdge)\n",
    "\n",
    "#Define a binary characteristic object\n",
    "class Binary_Characteristic:\n",
    "    def __init__(self, label, values):\n",
    "        self.label = label\n",
    "        self.values = values\n",
    "\n",
    "#Define stats and classifier and regression program\n",
    "def STATS_CART(printEverything, trainSplit, filename, classifierID, classifierName, shouldBalanceData, autoSampleSize, trainingSampleSize, oversample, synthetic, truncated_normal, normal):\n",
    "    #Import and split dataset\n",
    "    if (printEverything == 1): \n",
    "        print('ANALYSING DATASET')\n",
    "        print('\\n')\n",
    "    df = pd.read_csv(filename, na_values='?') #Read in dataset\n",
    "    df = df.astype('float64') #Convert all values to be the same data type\n",
    "\n",
    "    if (printEverything == 1): \n",
    "        print(df.info())\n",
    "        print('\\n')\n",
    "        print('GENERATING BASE DATASET STATISTICS')\n",
    "        print('\\n')\n",
    "        basestats = df.min().rename_axis('Variable').reset_index(name='Minimum')\n",
    "        basestats['Maximum'] = df.max().values\n",
    "        basestats['Average'] = df.mean().values\n",
    "        basestats['#Missing'] = df.isna().sum().values\n",
    "        basestats['%Missing']= ((basestats['#Missing']/len(df))*100).values\n",
    "        basestats['Classifier Correlation'] = df.corr()[classifierID].values\n",
    "        print(basestats)\n",
    "        print('\\n')\n",
    "        print('Total amount of data missing is', basestats['%Missing'].sum(),'%')\n",
    "        print('\\n')\n",
    "        print('Maximum amount of data missing for a single variable is', basestats['%Missing'].max(),'%')\n",
    "\n",
    "        print('\\n')\n",
    "        print('REMOVING NA VALUES')\n",
    "    df=df.dropna() #Remove all na.values\n",
    "    \n",
    "    if (printEverything == 1): \n",
    "        print('\\n')\n",
    "        print('DETERMINING TOTAL AND MAXIMUM DATA CORRELATIONS')\n",
    "        print('\\n')\n",
    "        f, ax = plt.subplots(figsize=(6,6))\n",
    "        sns.heatmap(df.corr(), fmt = \".1f\", ax = ax, annot = True, cmap=\"Greys\")\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        absmaxcorr = pd.DataFrame((df.corr()[df.corr() != 1]).abs().max().rename_axis('1st Variable').reset_index(name='Max Correlation'))\n",
    "        relatedCorrelations = [] #Create an empty list for matching correlation values\n",
    "        for i in range (0, len(absmaxcorr)): #For each of the variables \n",
    "            variable = absmaxcorr['Max Correlation'].loc[i] #Take its maximum correlation value\n",
    "            for column in df.corr().abs(): #For each of the columns in the correlation set\n",
    "                if (variable == df.corr().abs().iloc[i][column]): #If the value matches that being sought\n",
    "                    relatedCorrelations.append(column) #Add the column's name to the list\n",
    "        absmaxcorr['Cross Variable']=relatedCorrelations\n",
    "        print(absmaxcorr)\n",
    "\n",
    "    #Balance the data if indicated and graph frequency of classes\n",
    "    if (shouldBalanceData): #If the balance data option has been enabled\n",
    "        if (printEverything == 1): \n",
    "            if (oversample):\n",
    "                print('\\n')\n",
    "                print('DATA WILL BE RE-BALANCED USING OVERSAMPLING')\n",
    "            if (synthetic):\n",
    "                if (truncated_normal):\n",
    "                    print('\\n')\n",
    "                    print('DATA WILL BE RE-BALANCED USING SYNTHETIC SAMPLE GENERATION FROM A TRUNCATED NORMAL DISTRIBUTION')\n",
    "                if (normal):\n",
    "                    print('\\n')\n",
    "                    print('DATA WILL BE RE-BALANCED USING SYNTHETIC SAMPLE GENERATION FROM A NORMAL DISTRIBUTION')\n",
    "        if (autoSampleSize == 1): #If the sample size for rebalancing should be automatically determined\n",
    "            #Indicate such when calling the balanceData function\n",
    "            if (printEverything == 1): \n",
    "                print('\\n')\n",
    "                print('SAMPLE SIZE WILL BE SET AUTOMATICALLY')\n",
    "            final_df = balanceData(df, df, classifierID, -1, oversample, synthetic, truncated_normal, normal)#Use the training set to create a balanced class dataset\n",
    "        else: #Otherwise use the value specified by the user\n",
    "            if (printEverything == 1): \n",
    "                print('\\n')\n",
    "                print('SAMPLE SIZE WAS SET BY THE USER')\n",
    "            final_df = balanceData(df, df, classifierID, trainingSampleSize, oversample, synthetic, truncated_normal, normal) #Use the training set to create a balanced class dataset\n",
    "\n",
    "        if (printEverything == 1): \n",
    "            #Generate histogram for the frequency of classifier values in original set\n",
    "            print('\\n')\n",
    "            print('GENERATING DATASET FREQUENCY HISTOGRAM')\n",
    "            #Original\n",
    "            f, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,3), sharey=True)\n",
    "            labels, counts = np.unique(df[classifierID], return_counts=True)\n",
    "            ax1.bar(labels, counts, align='center', color='k')\n",
    "            ax1.set_title('Original')\n",
    "            ax1.set_xlabel(classifierName)\n",
    "            ax1.set_ylabel('Frequency')\n",
    "\n",
    "            #Generate histogram for the frequency of classifier values in the balanced dataset\n",
    "            #Balanced\n",
    "            labels, counts = np.unique(final_df[classifierID], return_counts=True)\n",
    "            ax2.bar(labels, counts, align='center', color='k')\n",
    "            ax2.set_title('Final')\n",
    "            ax2.set_xlabel(classifierName)\n",
    "            string='Frequency of '+classifierName\n",
    "            plt.suptitle(string)\n",
    "            plt.subplots_adjust(wspace=0, top=0.8)\n",
    "            plt.show()\n",
    "\n",
    "    else: #Otherwise dataset is entirely unbalanced\n",
    "        final_df = df.copy() #Copy original for use in final\n",
    "        if (printEverything == 1): \n",
    "            print('\\n')\n",
    "            print('GENERATING DATASET FREQUENCY HISTOGRAM')\n",
    "            #Generate histogram for the frequency of classifier values\n",
    "            labels, counts = np.unique(final_df[classifierID], return_counts=True)\n",
    "            plt.bar(labels, counts, align='center', color='k')\n",
    "            plt.title(\"Frequency of \"+classifierName)\n",
    "            plt.xlabel(classifierName)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "\n",
    "    #Allocate training/testing sets\n",
    "    if (printEverything == 1): \n",
    "        print('\\n')\n",
    "        print('ALLOCATING TRAINING/TESTING SETS')\n",
    "    df = shuffle(df) #Randomize original dataset prior to split\n",
    "    final_df = shuffle(final_df) #Randomize final dataset prior to split\n",
    "    numTrain = int(len(final_df.index)*(trainSplit/100)) #Find number of training examples; round as int for indexing\n",
    "    trainingData = final_df.iloc[0:numTrain] #Split off a training dataset using the balanced data\n",
    "    testData = df.iloc[numTrain:(len(df.index))] #Split off a test dataset from the original set regardless of balanced or not\n",
    "\n",
    "    if (printEverything == 1): \n",
    "        print('\\n')\n",
    "        print('DROPPING VARIABLES LESS THAN THE MEAN CORRELATION TO THE CLASSIFIER FROM THE TRAINING SET')\n",
    "    a = df.corr()[classifierID].abs().rename_axis('variable').reset_index(name='correlation')\n",
    "    a = a[a['variable']!=classifierID]\n",
    "    dropVariables = a[a['correlation'] <= a['correlation'].mean()]\n",
    "    keepVariables = a[a['correlation'] >= a['correlation'].mean()]\n",
    "    if (printEverything == 1): \n",
    "        print(dropVariables['variable'].values)\n",
    "        print('\\n')\n",
    "        print('KEEPING VARIABLES')\n",
    "        print(keepVariables['variable'].values)\n",
    "    trainingData = trainingData.drop(dropVariables['variable'].values, axis=1)\n",
    "\n",
    "    #For each characteristic, find the greatest information gain possible using Gini impurity\n",
    "    #Split the data adding that node to a tree\n",
    "    #Repeat until termination criteria and then print out the tree to tree.png\n",
    "\n",
    "    #Build tree\n",
    "    if (printEverything == 1): \n",
    "        print('\\n')\n",
    "        print('BUILDING TREE (THIS WILL TAKE A WHILE)\\n')\n",
    "    tree = Decision_Tree(trainingData, classifierID, 1, printEverything) #Construct a decision tree for determining class\n",
    "    finalGraph = buildGraph(tree) #Reconstruct full tree from descision tree object's sub-trees\n",
    "    finalGraph.write_png(classifierID+'_Decision_Tree.png') #Write the full tree to a file\n",
    "\n",
    "    #Evaluate Tree\n",
    "    print('\\n')\n",
    "    print('EVALUATING SUCCESS OF TREE FOR TESTING SET')\n",
    "    successes = test(tree, testData, classifierID)\n",
    "    print('\\nTest Samples:',len(testData),'\\nSuccesses:',successes,'\\nFailures:',len(testData)-successes)\n",
    "    print('Success Rate for Test Data:',round((successes/len(testData))*100,3),'%')\n",
    "    \n",
    "    #Write results to a file\n",
    "    outFilename = classifierName+'_train_'+str(trainSplit)+'_balanced_'+str(shouldBalanceData)+'_autoSamp_'+str(autoSampleSize)+'_manSampSize_'+str(trainingSampleSize)+'_OverSam_'+str(oversample)+'_SynthSam_'+str(synthetic)+'_trunNorm_'+str(truncated_normal)+'_normal_'+str(normal)+'.txt'\n",
    "    outData = 'Test Samples: '+str(len(testData))+'\\nSuccesses: '+str(successes)+'\\nFailures: '+str(len(testData)-successes)+'\\nSuccess Rate for Test Data: '+str(round((successes/len(testData))*100, 3))+'%'\n",
    "    file = open(outFilename,\"w\") \n",
    "    file.write(outData)\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "    if (printEverything == 1): \n",
    "        #Visualize the final tree\n",
    "        Image(filename=classifierID+'_Decision_Tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PROGRAM: STATS_CART MANUAL RUN\n",
    "\n",
    "#Specify program parameters for a manual run\n",
    "#NOTE: Program assumes the classifierID is located in the last column of the set!\n",
    "filename = 'wine.csv' #Indicate filename containing dataset\n",
    "classifierID = 'quality' #Indicate which variable should be predicted\n",
    "classifierName = 'Wine Quality' #Indicate variable name for graphs\n",
    "\n",
    "trainSplit = 0.1 #Indicate portion (%) of data to use for training; test is 1-trainSplit\n",
    "\n",
    "printEverything = 0 #(0: No, 1: Yes) Should anything but the accuracy results and tree building progress be printed\n",
    "\n",
    "shouldBalanceData = 1 #(0: No, 1: Yes) Should the data be re-balanced\n",
    "autoSampleSize = 1 #(0: No, 1: Yes) Should the sample size for each balanced class be set automatically\n",
    "#Automatic sample size: #samples/#cflassifiers\n",
    "trainingSampleSize=0 #If the sample size is not set to be determined automatically, specify how many samples are desired\n",
    "\n",
    "#Do not choose more than one of the following \n",
    "oversample = 1 #(0: No, 1: Yes) Should oversampling be used to compensate for class imbalance\n",
    "synthetic = 0 #(0: No, 1: Yes) Should synthetic, generated values be used to compensate for class imbalance\n",
    "\n",
    "#Do not choose more than one of the following and only select if synthetic is enabled\n",
    "truncated_normal = 0 #(0: No, 1: Yes) If synthetic, then should sample be taken from a truncated normal distribution\n",
    "normal = 0 #(0: No, 1: Yes) If synthetic, then should sample be taken from a truncated normal distribution\n",
    "\n",
    "#FUNCTION STATS_CART(printEverything, trainSplit, filename, classifierID, classifierName, shouldBalanceData, autoSampleSize, trainingSampleSize, oversample, synthetic, truncated_normal, normal):\n",
    "STATS_CART(printEverything, trainSplit, filename, classifierID, classifierName, shouldBalanceData, autoSampleSize, trainingSampleSize, oversample, synthetic, truncated_normal, normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PROGRAM: STATS_CART AUTOMATIC RUNS\n",
    "\n",
    "#Run all options for accuracy results only; autoSample and balanceData options are enabled\n",
    "filename = 'wine.csv' #Indicate filename containing dataset\n",
    "classifierID = 'quality' #Indicate which variable should be predicted\n",
    "classifierName = 'Wine Quality' #Indicate variable name for graphs\n",
    "\n",
    "#FUNCTION STATS_CART(printEverything, trainSplit, filename, classifierID, classifierName, shouldBalanceData, autoSampleSize, trainingSampleSize, oversample, synthetic, truncated_normal, normal)\n",
    "print('Oversampling, 0.1% Training')\n",
    "STATS_CART(0, 0.1, filename, classifierID, classifierName, 1, 1, 0, 1, 0, 0, 0)\n",
    "print('\\n\\n')\n",
    "print('Oversampling, 1% Training')\n",
    "STATS_CART(0, 1, filename, classifierID, classifierName, 1, 1, 0, 1, 0, 0, 0)\n",
    "print('\\n\\n')\n",
    "print('Oversampling, 10% Training')\n",
    "STATS_CART(0, 10, filename, classifierID, classifierName, 1, 1, 0, 1, 0, 0, 0)\n",
    "print('\\n\\n')\n",
    "print('Oversampling, 80% Training')\n",
    "STATS_CART(0, 80, filename, classifierID, classifierName, 1, 1, 0, 1, 0, 0, 0)\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Synthetic, 0.1% Training, Normal Sampling')\n",
    "STATS_CART(0, 0.1, filename, classifierID, classifierName, 1, 1, 0, 0, 1, 0, 1)\n",
    "print('\\n\\n')\n",
    "print('Synthetic, 1% Training, Normal Sampling')\n",
    "STATS_CART(0, 1, filename, classifierID, classifierName, 1, 1, 0, 0, 1, 0, 1)\n",
    "print('\\n\\n')\n",
    "print('Synthetic, 10% Training, Normal Sampling')\n",
    "STATS_CART(0, 10, filename, classifierID, classifierName, 1, 1, 0, 0, 1, 0, 1)\n",
    "print('\\n\\n')\n",
    "print('Synthetic, 80% Training, Normal Sampling')\n",
    "STATS_CART(0, 80, filename, classifierID, classifierName, 1, 1, 0, 0, 1, 0, 1)\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Synthetic, 0.1% Training, Truncated Normal Sampling')\n",
    "STATS_CART(0, 0.1, filename, classifierID, classifierName, 1, 1, 0, 0, 1, 1, 0)\n",
    "print('\\n\\n')\n",
    "print('Synthetic, 1% Training, Truncated Normal Sampling')\n",
    "STATS_CART(0, 1, filename, classifierID, classifierName, 1, 1, 0, 0, 1, 1, 0)\n",
    "print('\\n\\n')\n",
    "print('Synthetic, 10% Training, Truncated Normal Sampling')\n",
    "STATS_CART(0, 10, filename, classifierID, classifierName, 1, 1, 0, 0, 1, 1, 0)\n",
    "print('\\n\\n')\n",
    "print('Synthetic, 80% Training, Truncated Normal Sampling')\n",
    "STATS_CART(0, 80, filename, classifierID, classifierName, 1, 1, 0, 0, 1, 1, 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
